{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"TODO: Add a description here.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from dateutil import parser as date_parser\n",
    "\n",
    "# TODO: Add BibTeX citation\n",
    "# Find for instance the citation on arxiv or on the dataset repo/website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CITATION = \"\"\"\\\n",
    "@InProceedings{huggingface:dataset,\n",
    "title = {Mortality prediction based on discharge notes from MIMIC-III},\n",
    "authors={huggingface, Inc.\n",
    "},\n",
    "year={2020}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Add description of the dataset here\n",
    "# You can copy an official description\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "This is the mortality task explored in our paper on concept-level interpretability of models that process clinical notes. \n",
    "\"\"\"\n",
    "\n",
    "# TODO: Add a link to an official homepage for the dataset here\n",
    "_HOMEPAGE = \"https://github.com/amoldwin/Health-BERT-TCAV\"\n",
    "\n",
    "# TODO: Add the licence for the dataset here if you can find it\n",
    "_LICENSE = \"\"\n",
    "\n",
    "# TODO: Add link to the official dataset URLs here\n",
    "# The HuggingFace dataset library don't host the datasets but only point to the original files\n",
    "# This can be an arbitrary nested dict/list of URLs (see below in `_split_generators` method)\n",
    "_URLs = {\n",
    "    'first_domain': \"https://github.com/amoldwin/Health-BERT-TCAV\",\n",
    "#     'second_domain': \"https://huggingface.co/great-new-dataset-second_domain.zip\",\n",
    "}\n",
    "\n",
    "CALIBRATION = datasets.NamedSplit(\"calibration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Name of the dataset usually match the script name with CamelCase instead of snake_case\n",
    "class ClueDataset(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"TODO: Short description of my dataset.\"\"\"\n",
    "\n",
    "    VERSION = datasets.Version(\"1.1.0\")\n",
    "\n",
    "    # This is an example of a dataset with multiple configurations.\n",
    "    # If you don't want/need to define several sub-sets in your dataset,\n",
    "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
    "\n",
    "    # If you need to make complex sub-parts in the datasets with configurable options\n",
    "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\n",
    "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\n",
    "\n",
    "    # You will be able to load one or the other configurations in the following list with\n",
    "    # data = datasets.load_dataset('my_dataset', 'first_domain')\n",
    "    # data = datasets.load_dataset('my_dataset', 'second_domain')\n",
    "    BUILDER_CONFIGS = [\n",
    "        datasets.BuilderConfig(name=\"discharge_mortality\", version=VERSION,\n",
    "                               description=\"Mortality prediction based on discharge notes from MIMIC-III\"),\n",
    "    ]\n",
    "\n",
    "    DEFAULT_CONFIG_NAME = \"discharge_mortality_config\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\n",
    "\n",
    "    def _info(self):\n",
    "        features = {\n",
    "            \"hadm_id\": datasets.Value(\"int32\"),\n",
    "            \"timestamp\": datasets.Value(\"timestamp[s]\"),\n",
    "            \"text\": datasets.Value(\"string\"),\n",
    "            \"TEXT\": datasets.Value(\"string\"),\n",
    "            \"label\": datasets.Value(\"int8\"),\n",
    "        }\n",
    "        filepaths={}\n",
    "        \n",
    "        multitask = self.config.name == 'multitask'\n",
    "        if self.config.name == 'discharge_mortality':\n",
    "            features['discharge_mortality'] = datasets.Value(\"int8\")\n",
    "            filepaths['discharge_mortality']={'train': 'mortality_train.csv',\n",
    "                                             'val': 'mortality_val.csv',\n",
    "                                             'test': 'mortality_test.csv'}\n",
    "        features = datasets.Features(features)\n",
    "\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=_DESCRIPTION,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,  # Here we define them above because they are different between the two configurations\n",
    "            # If there's a common (input, target) tuple from the features,\n",
    "            # specify them here. They'll be used if as_supervised=True in\n",
    "            # builder.as_dataset.\n",
    "            supervised_keys=None,\n",
    "            # Homepage of the dataset for documentation\n",
    "            homepage=_HOMEPAGE,\n",
    "            # License for the dataset if available\n",
    "            license=_LICENSE,\n",
    "            # Citation for the dataset\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        \"\"\"Returns SplitGenerators.\"\"\"\n",
    "        # TODO: This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\n",
    "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\n",
    "\n",
    "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLs\n",
    "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\n",
    "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive \n",
    "        # my_urls = _URLs[self.config.name]\n",
    "        # data_dir = dl_manager.download_and_extract(my_urls)\n",
    "        data_dir = self.config.data_dir\n",
    "\n",
    "        # noinspection PyTypeChecker\n",
    "        return [\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TRAIN,\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, filepaths[self.config.name]['train']),\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.VALIDATION,\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, filepaths[self.config.name]['val']),\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=CALIBRATION,\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, filepaths[self.config.name]['val']),\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TEST,\n",
    "                gen_kwargs={\n",
    "                    \"filepath\": os.path.join(data_dir, filepaths[self.config.name]['test']),\n",
    "                },\n",
    "            ),\n",
    "\n",
    "        ]\n",
    "\n",
    "    def _get_label_parse_fn(self):\n",
    "        if self.config.name.startswith('discharge_mortality'):\n",
    "            def parse_fn(data):\n",
    "                return {f'discharge_mortality': int(float(data['label']))}\n",
    "            \n",
    "            \n",
    "        if self.config.name.startswith('staging'):\n",
    "            disease = self.config.name[8:]\n",
    "\n",
    "            def parse_fn(data):\n",
    "                return {f'{disease}_stage': int(float(data['label']))}\n",
    "\n",
    "        elif self.config.name == 'phenotyping':\n",
    "            def parse_fn(data):\n",
    "                phenotypes = [bool(data[f'{phenotype}']) for phenotype in constants.PHENOTYPE_LABELS]\n",
    "                return {'phenotypes': phenotypes}\n",
    "\n",
    "        elif self.config.name == 'mortality':\n",
    "            def parse_fn(data):\n",
    "                return {'mortality': data['label']}\n",
    "\n",
    "        elif self.config.name == 'length_of_stay':\n",
    "            def parse_fn(data):\n",
    "                return {'los': data['label']}\n",
    "        else:\n",
    "            assert self.config.name == 'multitask'\n",
    "\n",
    "            def parse_fn(data):\n",
    "                return {\n",
    "                    'aki_stage': int(data['aki_stage']),\n",
    "                    'anemia_stage': int(data['anemia_stage']),\n",
    "                    'phenotypes': [bool(data[f'ccs_{phenotype}']) for phenotype in constants.PHENOTYPE_LABELS],\n",
    "                    'pi_stage': int(data['pi_stage']),\n",
    "                    'mortality': data['mortality'],\n",
    "                    'los': data['los'],\n",
    "                }\n",
    "\n",
    "        return parse_fn\n",
    "\n",
    "    def _generate_examples(self, filepath):\n",
    "        \"\"\" Yields examples. \"\"\"\n",
    "        # TODO: This method will receive as arguments the `gen_kwargs` defined in the previous `_split_generators` method.\n",
    "        # It is in charge of opening the given file and yielding (key, example) tuples from the dataset\n",
    "        # The key is not important, it's more here for legacy reason (legacy from tfds)\n",
    "        data_df = pd.read_csv(filepath)\n",
    "        label_parse_fn = self._get_label_parse_fn()\n",
    "        for id_ in range(len(data_df)):\n",
    "            yield id_, data_df.iloc[id_].to_dict()\n",
    "            \n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            r = csv.DictReader(f, quoting=csv.QUOTE_NONE)\n",
    "            label_parse_fn = self._get_label_parse_fn()\n",
    "            for id_, row in enumerate(r):\n",
    "                yield id_, {\n",
    "                    'hadm_id': row['hadm_id'],\n",
    "                    'timestamp': date_parser.parse(row['timestamp']).timestamp(),\n",
    "                    'text': row['observations'],\n",
    "                    **label_parse_fn(row)\n",
    "                }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
