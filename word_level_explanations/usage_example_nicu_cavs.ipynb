{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from dataset import NewsDataset\n",
    "from BertModel import WeightedBertForSequenceClassification\n",
    "from BertModel import Bert_cutted_cosine\n",
    "\n",
    "\n",
    "from smooth_gradient import SmoothGradient\n",
    "from integrated_gradient import IntegratedGradient\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertConfig, DistilBertTokenizer\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cavs_dir='../torch_tcav/tcav_scripts/tcav_class_test_halfsplitFINALWeightedBlueBert512NicuLosNoneRemovedHalfValTest_1_checkpoint-2375_layer11/cavs/'\n",
    "# f1s_dct={}\n",
    "# for fn in os.listdir(cavs_dir):\n",
    "#     with open(os.path.join(cavs_dir,fn),'rb') as file:\n",
    "#         cav_dct = pickle.load(file)\n",
    "#     f1=float(cav_dct['metrics'].split()[12])\n",
    "#     f1s_dct[fn]=f1\n",
    "with open('../torch_tcav/tcav_scripts/tcav_class_test_halfsplitFINALWeightedBlueBert512NicuLosNoneRemovedHalfValTest_1_checkpoint-2375_layer11/cavs/positive_7742-negative500_7742_22-11-linear-0.1.pkl','rb') as file:\n",
    "    cav_dct = pickle.load(file)\n",
    "\n",
    "cav = cav_dct['cavs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jaundice_f1s={k:v for k,v in f1s_dct.items() if '7742' in k}\n",
    "# max(jaundice_f1s, key=jaundice_f1s.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       165\n",
      "           1       0.84      0.82      0.83       165\n",
      "\n",
      "    accuracy                           0.84       330\n",
      "   macro avg       0.84      0.84      0.84       330\n",
      "weighted avg       0.84      0.84      0.84       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cav_dct['metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../torch_tcav/model_train_scripts/checkpoints/FINALWeightedBlueBert512NicuLosNoneRemovedHalfValTest_1/checkpoint-2375 were not used when initializing WeightedBertForSequenceClassification: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing WeightedBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WeightedBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WeightedBertForSequenceClassification were not initialized from the model checkpoint at ../torch_tcav/model_train_scripts/checkpoints/FINALWeightedBlueBert512NicuLosNoneRemovedHalfValTest_1/checkpoint-2375 and are newly initialized: ['classifier.linear.weight', 'classifier.linear.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = '../torch_tcav/model_train_scripts/checkpoints/FINALWeightedBlueBert512NicuLosNoneRemovedHalfValTest_1/checkpoint-2375'\n",
    "bert = WeightedBertForSequenceClassification.from_pretrained(model_path,output_attentions=True,cav=cav,layer_number=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(['This is a test','so is this'], padding='max_length', max_length=512)\n",
    "outs=bert(input_ids=torch.LongTensor([tokens]),output_hidden_states=True,return_dict=True,return_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12567417, -0.00343427,  0.5475866 , ..., -0.52313054,\n",
       "         0.06466903, -0.96396595]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs['hidden_states'][12].detach().cpu().numpy().reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5008, 0.4992]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.classifier.to(bert.bert.device)\n",
    "bert.classifier.cav_tensor=bert.classifier.cav_tensor.to(bert.bert.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_list = ['IUI, trichorionic, triamniotic','Antepartum remarkable for ART conception with subsequent twinning of one embryo',\n",
    "              'Growth better than siblings, but still only around 10-25%','Infant TRIPLET BORN AT 31 WEEKS GESTATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads_df['ROW_ID'] = list(train_df['ROW_ID'])+list(val_df['ROW_ID'])+list(test_df['ROW_ID'])\n",
    "# grads_df['TEXT'] = list(train_df['TEXT'])+list(val_df['TEXT'])+list(test_df['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.96s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.86s/it]\n"
     ]
    }
   ],
   "source": [
    "texts=['this patient is very healthy','this patient has jaundice']\n",
    "config = BertConfig()\n",
    "batch_size=1\n",
    "all_instances = []\n",
    "for text in texts:\n",
    "    test_example = [[text],['']]\n",
    "\n",
    "    test_dataset = NewsDataset(\n",
    "        data_list=test_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=config.max_position_embeddings, \n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    integrated_grad = IntegratedGradient(\n",
    "        bert, \n",
    "        criterion, \n",
    "        tokenizer, \n",
    "        show_progress=True,\n",
    "        encoder=\"bert\"\n",
    "    )\n",
    "    instances = integrated_grad.saliency_interpret(test_dataloader)\n",
    "    all_instances.append(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"barcode\"; style=\"color: black; background-color: #f7fbff\"> [CLS]</span><span class=\"barcode\"; style=\"color: black; background-color: #b2d2e8\"> this</span><span class=\"barcode\"; style=\"color: black; background-color: #a1cbe2\"> patient</span><span class=\"barcode\"; style=\"color: black; background-color: #4997c9\"> is</span><span class=\"barcode\"; style=\"color: black; background-color: #08306b\"> very</span><span class=\"barcode\"; style=\"color: black; background-color: #4594c7\"> healthy</span><span class=\"barcode\"; style=\"color: black; background-color: 0\">    Label: 1 |</span><span class=\"barcode\"; style=\"color: black; background-color: #72c375\">50.49%</span>|"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coloder_string = integrated_grad.colorize({k:v[:-1] if type(v)==list else v for k,v in all_instances[0][0].items()})\n",
    "display(HTML(coloder_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokens': ['[CLS]',\n",
       "   'this',\n",
       "   'patient',\n",
       "   'is',\n",
       "   'a',\n",
       "   'triple',\n",
       "   '##t',\n",
       "   'with',\n",
       "   'ja',\n",
       "   '##und',\n",
       "   '##ice',\n",
       "   'and',\n",
       "   'the',\n",
       "   'weight',\n",
       "   'is',\n",
       "   'fine',\n",
       "   'and',\n",
       "   'siblings',\n",
       "   'are',\n",
       "   'also',\n",
       "   'fine',\n",
       "   '.',\n",
       "   '[SEP]'],\n",
       "  'grad': [0.01634272187948227,\n",
       "   0.03272367641329765,\n",
       "   0.02459438145160675,\n",
       "   0.014008548110723495,\n",
       "   0.01997995190322399,\n",
       "   0.11511045694351196,\n",
       "   0.035266708582639694,\n",
       "   0.00711166812106967,\n",
       "   0.0710081085562706,\n",
       "   0.024661462754011154,\n",
       "   0.05291587486863136,\n",
       "   0.004441768862307072,\n",
       "   0.0391855426132679,\n",
       "   0.04922395199537277,\n",
       "   0.020197290927171707,\n",
       "   0.007337586022913456,\n",
       "   0.05729655176401138,\n",
       "   0.042740028351545334,\n",
       "   0.03723704069852829,\n",
       "   0.009259200654923916,\n",
       "   0.02745366096496582,\n",
       "   0.03163047134876251,\n",
       "   0.12183572351932526],\n",
       "  'label': 1,\n",
       "  'prob': 0.5070853233337402}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_instances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['[CLS]',\n",
       "  'this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'test',\n",
       "  'in',\n",
       "  'which',\n",
       "  'the',\n",
       "  'the',\n",
       "  'pregnancy',\n",
       "  'were',\n",
       "  'under',\n",
       "  '##weight',\n",
       "  'triple',\n",
       "  '##ts',\n",
       "  'and',\n",
       "  'pneumonia',\n",
       "  'and',\n",
       "  'ja',\n",
       "  '##und',\n",
       "  '##ice',\n",
       "  '.',\n",
       "  '[SEP]'],\n",
       " 'grad': [0.01483505591750145,\n",
       "  0.0071844058111310005,\n",
       "  0.0031794332899153233,\n",
       "  0.006486645899713039,\n",
       "  0.013977427035570145,\n",
       "  0.0027292438317090273,\n",
       "  0.009400671347975731,\n",
       "  0.026495056226849556,\n",
       "  0.021737011149525642,\n",
       "  0.009523723274469376,\n",
       "  0.02988959290087223,\n",
       "  0.024764586240053177,\n",
       "  0.020968571305274963,\n",
       "  0.07252927869558334,\n",
       "  0.05242902785539627,\n",
       "  0.036799315363168716,\n",
       "  0.01895962469279766,\n",
       "  0.04123625159263611,\n",
       "  0.0461561493575573,\n",
       "  0.03622741997241974,\n",
       "  0.08125228434801102,\n",
       "  0.042408913373947144,\n",
       "  0.24532140791416168],\n",
       " 'label': 1,\n",
       " 'prob': 0.7216453552246094}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads_df = pd.read_csv('all_nicu_grads_CAV_positive_V3401-negative500_V3401_40-11-linear-0.1.pkl.csv')\n",
    "grads_df = pd.read_csv('all_nicu_grads_CAV_positive_7742-negative500_7742_22-11-linear-0.1.pkl.csv')\n",
    "\n",
    "grads_df['tokens']=grads_df['tokens'].apply(eval)\n",
    "\n",
    "grads_df['grad'] = grads_df['grad'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel(lst):\n",
    "    return [i for j in lst for i in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = 1\n",
    "all_class_grads = pd.DataFrame(  list(zip( unravel(grads_df['tokens']),  unravel(grads_df['grad']) )) , columns=['tokens', 'grad']  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word =''\n",
    "score=0\n",
    "words=[]\n",
    "scores=[]\n",
    "for i in reversed(range(len(all_class_grads))):\n",
    "    token=all_class_grads['tokens'].iloc[i]\n",
    "    if token.startswith('##'):\n",
    "        word = token[2:]+word\n",
    "        score = max(score,all_class_grads['grad'].iloc[i] )\n",
    "#         print(word)\n",
    "    else:\n",
    "        word = token+word\n",
    "#         print(word)\n",
    "        words.append(word)\n",
    "        score = max(score,all_class_grads['grad'].iloc[i] )\n",
    "        scores.append(score)\n",
    "        word=''\n",
    "        score=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5759\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>grad_mean</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>7597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13268</th>\n",
       "      <td>hbsab</td>\n",
       "      <td>0.038641</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8575</th>\n",
       "      <td>babycares</td>\n",
       "      <td>0.032463</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>agpars</td>\n",
       "      <td>0.032443</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>300mmhg</td>\n",
       "      <td>0.025502</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21169</th>\n",
       "      <td>tcpo2</td>\n",
       "      <td>0.023884</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21133</th>\n",
       "      <td>tags</td>\n",
       "      <td>0.020383</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15665</th>\n",
       "      <td>minutes</td>\n",
       "      <td>0.019389</td>\n",
       "      <td>1564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21173</th>\n",
       "      <td>teaching</td>\n",
       "      <td>0.017830</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21166</th>\n",
       "      <td>tco2</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15504</th>\n",
       "      <td>membrane</td>\n",
       "      <td>0.017718</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8558</th>\n",
       "      <td>azt</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22506</th>\n",
       "      <td>vss</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12493</th>\n",
       "      <td>flovent</td>\n",
       "      <td>0.017279</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>mycin</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18120</th>\n",
       "      <td>premie</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7907</th>\n",
       "      <td>antepartal</td>\n",
       "      <td>0.016626</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14151</th>\n",
       "      <td>indocin</td>\n",
       "      <td>0.016578</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18002</th>\n",
       "      <td>practice</td>\n",
       "      <td>0.016275</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20062</th>\n",
       "      <td>sib</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17263</th>\n",
       "      <td>parenting</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13760</th>\n",
       "      <td>hyperoxia</td>\n",
       "      <td>0.014820</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
       "      <td>corrected</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8783</th>\n",
       "      <td>betamethasone</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17328</th>\n",
       "      <td>pathway</td>\n",
       "      <td>0.014239</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16581</th>\n",
       "      <td>nursery</td>\n",
       "      <td>0.014073</td>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8772</th>\n",
       "      <td>betacomplete</td>\n",
       "      <td>0.013795</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18030</th>\n",
       "      <td>precipitously</td>\n",
       "      <td>0.013668</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>adhesions</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13669</th>\n",
       "      <td>hyaline</td>\n",
       "      <td>0.013264</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10589</th>\n",
       "      <td>dcereased</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10618</th>\n",
       "      <td>decended</td>\n",
       "      <td>0.012608</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7404</th>\n",
       "      <td>addendum</td>\n",
       "      <td>0.012572</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20883</th>\n",
       "      <td>surfactant</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8770</th>\n",
       "      <td>beta</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16551</th>\n",
       "      <td>nsy</td>\n",
       "      <td>0.012201</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9445</th>\n",
       "      <td>cbcd</td>\n",
       "      <td>0.012153</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22409</th>\n",
       "      <td>vigorus</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19396</th>\n",
       "      <td>rhogam</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19854</th>\n",
       "      <td>secured</td>\n",
       "      <td>0.011792</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6281</th>\n",
       "      <td>6cmh2o</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8574</th>\n",
       "      <td>babycare</td>\n",
       "      <td>0.011517</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10333</th>\n",
       "      <td>course</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>2195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>346</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>settled</td>\n",
       "      <td>0.011070</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22465</th>\n",
       "      <td>vitk</td>\n",
       "      <td>0.010960</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9448</th>\n",
       "      <td>cbg</td>\n",
       "      <td>0.010935</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123</th>\n",
       "      <td>3650</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13989</th>\n",
       "      <td>immunization</td>\n",
       "      <td>0.010844</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13188</th>\n",
       "      <td>guidelines</td>\n",
       "      <td>0.010839</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  grad_mean  word_count\n",
       "7160           [SEP]   0.069539        7597\n",
       "13268          hbsab   0.038641           6\n",
       "8575       babycares   0.032463          26\n",
       "7595          agpars   0.032443          18\n",
       "3295         300mmhg   0.025502           6\n",
       "21169          tcpo2   0.023884           6\n",
       "21133           tags   0.020383         114\n",
       "15665        minutes   0.019389        1564\n",
       "21173       teaching   0.017830          19\n",
       "21166           tco2   0.017804          16\n",
       "15504       membrane   0.017718         323\n",
       "8558             azt   0.017570          13\n",
       "22506            vss   0.017333         331\n",
       "12493        flovent   0.017279          11\n",
       "15994          mycin   0.017219          16\n",
       "18120         premie   0.017200          75\n",
       "7907      antepartal   0.016626          12\n",
       "14151        indocin   0.016578           6\n",
       "18002       practice   0.016275           7\n",
       "20062            sib   0.015090          13\n",
       "17263      parenting   0.014945          40\n",
       "13760      hyperoxia   0.014820          41\n",
       "10298      corrected   0.014728           7\n",
       "8783   betamethasone   0.014399         735\n",
       "17328        pathway   0.014239          50\n",
       "16581        nursery   0.014073        1421\n",
       "8772    betacomplete   0.013795          10\n",
       "18030  precipitously   0.013668          12\n",
       "7434       adhesions   0.013660           7\n",
       "13669        hyaline   0.013264         295\n",
       "10589      dcereased   0.013064           6\n",
       "10618       decended   0.012608          29\n",
       "7404        addendum   0.012572          96\n",
       "20883     surfactant   0.012514         777\n",
       "8770            beta   0.012296         261\n",
       "16551            nsy   0.012201          11\n",
       "9445            cbcd   0.012153           6\n",
       "22409        vigorus   0.011995          11\n",
       "19396         rhogam   0.011908         131\n",
       "19854        secured   0.011792          16\n",
       "6281          6cmh2o   0.011680           6\n",
       "8574        babycare   0.011517          15\n",
       "10333         course   0.011355        2195\n",
       "3873             346   0.011312           9\n",
       "19970        settled   0.011070           8\n",
       "22465           vitk   0.010960           8\n",
       "9448             cbg   0.010935          15\n",
       "4123            3650   0.010855          11\n",
       "13989   immunization   0.010844           6\n",
       "13188     guidelines   0.010839          10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_grouped = pd.DataFrame(list(zip(words, scores)), columns=['words', 'grad']).groupby('words')\n",
    "means = tokens_grouped['grad'].mean()\n",
    "cts = tokens_grouped['grad'].count()\n",
    "wrds = tokens_grouped.apply(lambda group: str(group.name))\n",
    "class_df = pd.DataFrame(list(zip(wrds,means,cts)), columns=['word','grad_mean','word_count' ]).sort_values(by='grad_mean',ascending=False)\n",
    "class_df = class_df[class_df['word_count']>5]\n",
    "print(len(class_df))\n",
    "# print(class_df.head(25).to_latex(index=False, float_format=\"%.3f\"))\n",
    "class_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_negative_grads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-85266df71ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnegative_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnegative_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_negative_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_negative_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'##'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_negative_grads' is not defined"
     ]
    }
   ],
   "source": [
    "word =''\n",
    "score=0\n",
    "negative_words=[]\n",
    "negative_scores=[]\n",
    "for i in reversed(range(len(all_negative_grads))):\n",
    "    token=all_negative_grads['tokens'].iloc[i]\n",
    "    if token.startswith('##'):\n",
    "        word = token[2:]+word\n",
    "        score = max(score,all_negative_grads['grad'].iloc[i] )\n",
    "#         print(word)\n",
    "    else:\n",
    "        word = token+word\n",
    "#         print(word)\n",
    "        negative_words.append(word)\n",
    "        score = max(score,all_negative_grads['grad'].iloc[i] )\n",
    "        negative_scores.append(score)\n",
    "        word=''\n",
    "        score=0\n",
    "\n",
    "tokens_grouped = pd.DataFrame(list(zip(negative_words, negative_scores)), columns=['words', 'grad']).groupby('words')\n",
    "means = tokens_grouped['grad'].mean()\n",
    "cts = tokens_grouped['grad'].count()\n",
    "wrds = tokens_grouped.apply(lambda group: str(group.name))\n",
    "negative_df = pd.DataFrame(list(zip(wrds,means,cts)), columns=['word','grad_mean','word_count' ]).sort_values(by='grad_mean',ascending=False)\n",
    "negative_df = negative_df[negative_df['word_count']>5]\n",
    "print(negative_df.head(25).to_latex(index=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
