{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e92af70-5362-4167-a703-7d82714e27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import roc_auc_score, classification_report,average_precision_score\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix, recall_score, accuracy_score\n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a299b-64be-44dc-9567-dae58febd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs_dct = dict(list(pd.read_csv('../directory_paths.csv')['paths'].apply(eval)))\n",
    "checkpoints_dir = dirs_dct['checkpoints_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ecaf18-9307-4c8b-8b7f-5e544f6a5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e43e82-8040-428a-ad42-df3996027ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_tokenizer =  BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "943f5195-fdba-4277-b0e6-df39ea02a6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i =196\n",
    "# print(bio_tokenizer.vocab[i], bio_tokenizer.vocab[i])\n",
    "bio_tokenizer.encode('this is a test hypertension sentence') ==bert_tokenizer.encode('this is a test hypertension sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b2a714-87c7-4d3f-98af-634ac826fc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment name is FINALLogistic_0\n"
     ]
    }
   ],
   "source": [
    "EXP_NAME = 'FINALLogistic'\n",
    "\n",
    "# wandb.init(settings=wandb.Settings(start_method=\"fork\"))\n",
    "\n",
    "for i in range(100):\n",
    "    if EXP_NAME +'_'+str(i)+'.csv' not in os.listdir(checkpoints_dir):\n",
    "        EXP_NAME = EXP_NAME +'_'+str(i)\n",
    "        break\n",
    "print('experiment name is '+ EXP_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7e3b52-b128-40d4-b8b4-93fe40be7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(data):\n",
    "    encoded =  bert_tokenizer.encode_plus(\n",
    "            text=data,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            return_token_type_ids = True,\n",
    "            truncation=True\n",
    "            )\n",
    "    encoded=encoded['input_ids']\n",
    "    encoded = encoded \n",
    "    encoded = encoded + [bert_tokenizer.pad_token_id]*(512-len(encoded))\n",
    "    return np.asarray(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4b7ac4-4461-439f-9d7d-391fcae391b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(dirs_dct['data_dir'],'los_nicu_train_admissions.csv'))\n",
    "val_df = pd.read_csv(os.path.join(dirs_dct['data_dir'],'los_nicu_val_admissions.csv'))\n",
    "test_df = pd.read_csv(os.path.join(dirs_dct['data_dir'],'los_nicu_test_admissions.csv'))\n",
    "\n",
    "train_df['encoded'] = train_df['TEXT'].apply(bert_encode)\n",
    "val_df['encoded'] = val_df['TEXT'].apply(bert_encode)\n",
    "\n",
    "test_df['encoded'] = test_df['TEXT'].apply(bert_encode)\n",
    "\n",
    "train_df['TEXT']= train_df['encoded'].apply(lambda x: bert_tokenizer.decode(x))\n",
    "\n",
    "val_df['TEXT']= val_df['encoded'].apply(lambda x: bert_tokenizer.decode(x))\n",
    "\n",
    "test_df['TEXT']= test_df['encoded'].apply(lambda x: bert_tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f0bf76-b2e3-4cf3-afa5-60e32e6d7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(encoded_text):\n",
    "    bow = [0]*len(bert_tokenizer.vocab)\n",
    "    for x in encoded_text:\n",
    "        bow[x]=1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed09bcd7-1e05-486a-a535-4b583c692834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_tokenizer.vocab.values())\n",
    "# list(bert_tokenizer.vocab.values())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a9b2c2-5e52-448a-a634-522ad393515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_df['encoded'].apply(create_bag_of_words)\n",
    "val_inputs = val_df['encoded'].apply(create_bag_of_words)\n",
    "test_inputs = test_df['encoded'].apply(create_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "252eacc7-8016-4a01-b10c-1852732c9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = np.stack(train_inputs.values)\n",
    "val_inputs = np.stack(val_inputs.values)\n",
    "test_inputs = np.stack(test_inputs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a452f2-e1e6-4550-a9be-b91943bcfae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Defining neural network structure\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # needs to be done everytime in the nn.module derived class\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that are needed for linear model ( Ax + b)\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec): # Defines the computation performed at every call.\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42425d11-a118-4deb-b65a-07a0280510aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "epochs = n_iters / (len(train_inputs) / batch_size)\n",
    "input_dim = len(bert_tokenizer.vocab)\n",
    "output_dim = 2\n",
    "lr_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81fb7c5d-aa70-4f21-866e-1a5adaa9f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_nn_model = BoWClassifier(output_dim, input_dim)\n",
    "# bow_nn_model.to(device)\n",
    "\n",
    "# Loss Function\n",
    "loss_function = nn.NLLLoss()\n",
    "# Optimizer initlialization\n",
    "optimizer = optim.SGD(bow_nn_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5bdaee5-a769-42fa-93fe-894461183be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0de230af-f29d-42e5-80e6-6380069982d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(zip(train_inputs, train_df['label']))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e2fc724-f25f-4bbb-bbc2-639bbdfc7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = list(zip(val_inputs, val_df['label']))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = list(zip(test_inputs, test_df['label']))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d7e4446-2f15-451c-96e2-977677bca112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, save_dir, save_prefix, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f545763-febb-47a1-bf08-1c8908c33bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Loss: 0.4293558895587921. Accuracy: 74.61660766601562. MCC: 0.1579019072146128\n",
      "saving 0\n",
      "Iteration: 1. Loss: 0.4160304367542267. Accuracy: 81.54415893554688. MCC: 0.5348744569110886\n",
      "saving 1\n",
      "Iteration: 2. Loss: 0.18994273245334625. Accuracy: 82.12586212158203. MCC: 0.5441962458449998\n",
      "saving 2\n",
      "Iteration: 3. Loss: 0.35016241669654846. Accuracy: 83.9238510131836. MCC: 0.5723336075727167\n",
      "saving 3\n",
      "Iteration: 4. Loss: 0.7666645646095276. Accuracy: 78.90005493164062. MCC: 0.4321800310385275\n",
      "Iteration: 5. Loss: 0.4497051239013672. Accuracy: 82.39026641845703. MCC: 0.5644724433513655\n",
      "Iteration: 6. Loss: 0.28951457142829895. Accuracy: 85.56319427490234. MCC: 0.6211892558732252\n",
      "saving 6\n",
      "Iteration: 7. Loss: 0.3974981904029846. Accuracy: 85.72183990478516. MCC: 0.611922224214253\n",
      "Iteration: 8. Loss: 0.0648617148399353. Accuracy: 83.76520538330078. MCC: 0.5880767930077943\n",
      "Iteration: 9. Loss: 0.13615325093269348. Accuracy: 84.29402160644531. MCC: 0.5854980356375117\n",
      "Iteration: 10. Loss: 0.10129398852586746. Accuracy: 85.1930160522461. MCC: 0.5955437126465687\n",
      "Iteration: 11. Loss: 0.16856315732002258. Accuracy: 84.76996612548828. MCC: 0.6067112431934175\n",
      "Iteration: 12. Loss: 0.05608166754245758. Accuracy: 85.72183990478516. MCC: 0.6068202410948419\n",
      "Iteration: 13. Loss: 0.21625719964504242. Accuracy: 85.0872573852539. MCC: 0.6034954587619779\n",
      "Iteration: 14. Loss: 0.3549278676509857. Accuracy: 84.39978790283203. MCC: 0.5900960311706217\n",
      "Iteration: 15. Loss: 0.42055562138557434. Accuracy: 83.9238510131836. MCC: 0.5789996629721941\n",
      "Iteration: 16. Loss: 0.10163052380084991. Accuracy: 85.14013671875. MCC: 0.6056169293628704\n",
      "Iteration: 17. Loss: 0.2972447872161865. Accuracy: 86.1448974609375. MCC: 0.6121117975829622\n",
      "Iteration: 18. Loss: 0.2769756317138672. Accuracy: 85.61607360839844. MCC: 0.6283298545731638\n",
      "saving 18\n",
      "Iteration: 19. Loss: 0.2815130352973938. Accuracy: 86.03913116455078. MCC: 0.6135677112037867\n",
      "Iteration: 20. Loss: 0.13533075153827667. Accuracy: 85.88048553466797. MCC: 0.615729418114876\n",
      "Iteration: 21. Loss: 0.21720737218856812. Accuracy: 85.98625183105469. MCC: 0.6159984084666915\n",
      "Iteration: 22. Loss: 0.03986427187919617. Accuracy: 85.35166931152344. MCC: 0.6163305414825143\n",
      "Iteration: 23. Loss: 0.1139797493815422. Accuracy: 85.45742797851562. MCC: 0.6228277837298746\n",
      "Iteration: 24. Loss: 0.06973376870155334. Accuracy: 85.72183990478516. MCC: 0.6133309530315083\n",
      "Iteration: 25. Loss: 0.014079432003200054. Accuracy: 85.82760620117188. MCC: 0.6153729340541739\n",
      "Iteration: 26. Loss: 0.29669931530952454. Accuracy: 84.82284545898438. MCC: 0.596496799781126\n",
      "Iteration: 27. Loss: 0.34368962049484253. Accuracy: 85.66896057128906. MCC: 0.6153087055554924\n",
      "Iteration: 28. Loss: 0.5384631156921387. Accuracy: 85.77471923828125. MCC: 0.6271868251727247\n",
      "Iteration: 29. Loss: 0.011728769168257713. Accuracy: 85.88048553466797. MCC: 0.6227183688785575\n",
      "Iteration: 30. Loss: 0.17010948061943054. Accuracy: 85.51031494140625. MCC: 0.6193991457546011\n",
      "Iteration: 31. Loss: 0.35535895824432373. Accuracy: 86.03913116455078. MCC: 0.618140769908548\n",
      "Iteration: 32. Loss: 0.1073276698589325. Accuracy: 85.35166931152344. MCC: 0.6191443393125045\n",
      "Iteration: 33. Loss: 0.0705270916223526. Accuracy: 85.51031494140625. MCC: 0.6177199554683808\n",
      "Iteration: 34. Loss: 0.14888016879558563. Accuracy: 85.35166931152344. MCC: 0.6209325497814394\n",
      "Iteration: 35. Loss: 0.10806731879711151. Accuracy: 85.88048553466797. MCC: 0.618243395394868\n",
      "Iteration: 36. Loss: 0.04942602664232254. Accuracy: 85.82760620117188. MCC: 0.618243395394868\n",
      "Iteration: 37. Loss: 0.22253066301345825. Accuracy: 86.1448974609375. MCC: 0.6183000023211652\n",
      "Iteration: 38. Loss: 0.2954079508781433. Accuracy: 85.51031494140625. MCC: 0.6109871788572598\n",
      "Iteration: 39. Loss: 0.12536196410655975. Accuracy: 85.1930160522461. MCC: 0.6007722847137442\n",
      "Iteration: 40. Loss: 0.15214847028255463. Accuracy: 85.98625183105469. MCC: 0.6251349387425195\n",
      "Iteration: 41. Loss: 0.06832335889339447. Accuracy: 85.77471923828125. MCC: 0.6217372083416068\n",
      "Iteration: 42. Loss: 0.14479438960552216. Accuracy: 85.77471923828125. MCC: 0.6232774366342823\n",
      "Iteration: 43. Loss: 0.16166049242019653. Accuracy: 85.56319427490234. MCC: 0.6258942074310606\n",
      "Iteration: 44. Loss: 0.28779280185699463. Accuracy: 85.61607360839844. MCC: 0.6158107215830558\n",
      "Iteration: 45. Loss: 0.2523854970932007. Accuracy: 85.77471923828125. MCC: 0.6199362502023259\n",
      "Iteration: 46. Loss: 0.32650256156921387. Accuracy: 85.61607360839844. MCC: 0.6106543520764911\n",
      "Iteration: 47. Loss: 0.050889838486909866. Accuracy: 85.82760620117188. MCC: 0.6149809587062314\n",
      "Iteration: 48. Loss: 0.09002882242202759. Accuracy: 85.82760620117188. MCC: 0.6279180710686086\n",
      "Iteration: 49. Loss: 0.46952059864997864. Accuracy: 85.1930160522461. MCC: 0.5920289005251901\n",
      "Iteration: 50. Loss: 0.35849475860595703. Accuracy: 85.82760620117188. MCC: 0.6455703115810663\n",
      "saving 50\n",
      "Iteration: 51. Loss: 0.1399930715560913. Accuracy: 85.77471923828125. MCC: 0.6427637638600893\n",
      "Iteration: 52. Loss: 0.14427310228347778. Accuracy: 85.93336486816406. MCC: 0.613697709989837\n",
      "Iteration: 53. Loss: 0.06185581535100937. Accuracy: 85.98625183105469. MCC: 0.6232774366342823\n",
      "Iteration: 54. Loss: 0.5787906646728516. Accuracy: 85.93336486816406. MCC: 0.6162759460443984\n",
      "Iteration: 55. Loss: 0.21500512957572937. Accuracy: 85.98625183105469. MCC: 0.643679600691833\n",
      "Iteration: 56. Loss: 0.0231514573097229. Accuracy: 85.93336486816406. MCC: 0.6451135343802068\n",
      "Iteration: 57. Loss: 0.040727440267801285. Accuracy: 85.98625183105469. MCC: 0.6123523917626834\n",
      "Iteration: 58. Loss: 0.36184272170066833. Accuracy: 85.35166931152344. MCC: 0.6117432219095095\n",
      "Iteration: 59. Loss: 0.24477896094322205. Accuracy: 85.66896057128906. MCC: 0.6116434638326819\n",
      "Iteration: 60. Loss: 0.10502345114946365. Accuracy: 86.03913116455078. MCC: 0.6262047410079747\n",
      "Iteration: 61. Loss: 0.22318774461746216. Accuracy: 85.56319427490234. MCC: 0.6024690720800289\n",
      "Iteration: 62. Loss: 0.33679476380348206. Accuracy: 85.72183990478516. MCC: 0.624112775073619\n",
      "Iteration: 63. Loss: 0.4136675000190735. Accuracy: 85.77471923828125. MCC: 0.6461103632751769\n",
      "saving 63\n",
      "Iteration: 64. Loss: 0.22344696521759033. Accuracy: 85.77471923828125. MCC: 0.6257872272828048\n",
      "Iteration: 65. Loss: 0.7879525423049927. Accuracy: 84.24114227294922. MCC: 0.5624351112321845\n",
      "Iteration: 66. Loss: 0.2032421976327896. Accuracy: 85.56319427490234. MCC: 0.599432938139296\n",
      "Iteration: 67. Loss: 0.3618564307689667. Accuracy: 85.03437042236328. MCC: 0.5852929790372787\n",
      "Iteration: 68. Loss: 0.04111886024475098. Accuracy: 86.0920181274414. MCC: 0.6312192101287463\n",
      "Iteration: 69. Loss: 0.14314769208431244. Accuracy: 85.88048553466797. MCC: 0.6220907362442231\n",
      "Iteration: 70. Loss: 0.07149213552474976. Accuracy: 85.93336486816406. MCC: 0.6209325497814394\n",
      "Iteration: 71. Loss: 0.16158464550971985. Accuracy: 85.77471923828125. MCC: 0.617197564214962\n",
      "Iteration: 72. Loss: 0.12774422764778137. Accuracy: 85.66896057128906. MCC: 0.6446316641098281\n",
      "Iteration: 73. Loss: 0.008370853960514069. Accuracy: 85.98625183105469. MCC: 0.624112775073619\n",
      "Iteration: 74. Loss: 0.23521389067173004. Accuracy: 85.66896057128906. MCC: 0.6491050030622069\n",
      "saving 74\n",
      "Iteration: 75. Loss: 0.03451498970389366. Accuracy: 85.98625183105469. MCC: 0.6246652395484693\n",
      "Iteration: 76. Loss: 0.010191509500145912. Accuracy: 86.0920181274414. MCC: 0.6278775696899351\n",
      "Iteration: 77. Loss: 0.3986269235610962. Accuracy: 85.51031494140625. MCC: 0.6438856274942842\n",
      "Iteration: 78. Loss: 0.1652049720287323. Accuracy: 85.98625183105469. MCC: 0.6087749676861913\n",
      "Iteration: 79. Loss: 0.15323631465435028. Accuracy: 85.98625183105469. MCC: 0.6070857724299625\n",
      "Iteration: 80. Loss: 0.056762535125017166. Accuracy: 86.03913116455078. MCC: 0.6205559294112379\n",
      "Iteration: 81. Loss: 0.1165294349193573. Accuracy: 85.88048553466797. MCC: 0.5978672415920042\n",
      "Iteration: 82. Loss: 0.15132595598697662. Accuracy: 86.1977767944336. MCC: 0.6316988971313414\n",
      "Iteration: 83. Loss: 0.10584712028503418. Accuracy: 86.25066375732422. MCC: 0.6350356656595485\n",
      "Iteration: 84. Loss: 0.5451838374137878. Accuracy: 85.40454864501953. MCC: 0.6522267648383037\n",
      "saving 84\n",
      "Iteration: 85. Loss: 0.17641493678092957. Accuracy: 85.82760620117188. MCC: 0.6158936579461092\n",
      "Iteration: 86. Loss: 0.195949986577034. Accuracy: 85.98625183105469. MCC: 0.6097198838553047\n",
      "Iteration: 87. Loss: 0.0328761488199234. Accuracy: 86.1448974609375. MCC: 0.6287194443706471\n",
      "Iteration: 88. Loss: 0.3812989592552185. Accuracy: 85.35166931152344. MCC: 0.5937358969485075\n",
      "Iteration: 89. Loss: 0.09754867851734161. Accuracy: 86.1977767944336. MCC: 0.6456040060466047\n",
      "Iteration: 90. Loss: 0.14550144970417023. Accuracy: 85.61607360839844. MCC: 0.6042431473007248\n",
      "Iteration: 91. Loss: 0.21947221457958221. Accuracy: 86.1977767944336. MCC: 0.6427433014005989\n",
      "Iteration: 92. Loss: 0.19809342920780182. Accuracy: 85.66896057128906. MCC: 0.6501398450525636\n",
      "Iteration: 93. Loss: 0.046544987708330154. Accuracy: 86.3564224243164. MCC: 0.6362643190463049\n",
      "Iteration: 94. Loss: 0.38405799865722656. Accuracy: 86.1448974609375. MCC: 0.629549055652792\n",
      "Iteration: 95. Loss: 0.04997727647423744. Accuracy: 86.1448974609375. MCC: 0.6266390638982167\n",
      "Iteration: 96. Loss: 0.06169349327683449. Accuracy: 86.1977767944336. MCC: 0.6366423165332388\n",
      "Iteration: 97. Loss: 0.06059882044792175. Accuracy: 86.25066375732422. MCC: 0.6280108033886617\n",
      "Iteration: 98. Loss: 0.21185338497161865. Accuracy: 86.0920181274414. MCC: 0.6496122375044999\n",
      "Iteration: 99. Loss: 0.3312498927116394. Accuracy: 85.56319427490234. MCC: 0.5906346567170482\n",
      "Time taken to train the model: 140.47284364700317\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "best_mcc = 0\n",
    "for epoch in range(100):\n",
    "    for i, (ex, label) in enumerate(train_loader):\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        bow_nn_model.zero_grad()\n",
    "\n",
    "        # Step 2. Make BOW vector for input features and target label\n",
    "        bow_vec = ex.float()\n",
    "        target = label\n",
    "        \n",
    "        # Step 3. Run the forward pass.\n",
    "        probs = bow_nn_model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%1==0:\n",
    "        # calculate Accuracy\n",
    "        metrics_dict={}\n",
    "        for name, data_iter in {'val_':val_loader, 'test_':test_loader}.items():        \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            all_targets = []\n",
    "            all_preds = []\n",
    "            all_probs = []\n",
    "            for i, (ex, label) in enumerate(data_iter):\n",
    "                bow_vec = ex.float()\n",
    "                target = label\n",
    "                probs = bow_nn_model(bow_vec)\n",
    "\n",
    "                _, predicted = torch.max(probs.data, 1)\n",
    "                total+= target.size(0)\n",
    "                all_targets = all_targets + list(target)\n",
    "                all_preds = all_preds + list(predicted)\n",
    "                all_probs = all_probs + probs.tolist()\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == target).sum()\n",
    "            accuracy = 100 * correct/total\n",
    "            \n",
    "            metrics_dict[name+'accuracy'] = accuracy\n",
    "            metrics_dict[name+'mcc'] = matthews_corrcoef(all_targets, all_preds)\n",
    "            probs=sigmoid(all_probs)\n",
    "            all_targets_2d = [[1,0] if x==0 else [0,1] for x in all_targets]\n",
    "            metrics_dict[name+'accuracy'] =  accuracy_score(all_targets, all_preds)\n",
    "            # print(all_targets)\n",
    "            # print(all_preds)\n",
    "            metrics_dict[name+'auc_avg'] = roc_auc_score(all_targets_2d, sigm)\n",
    "            metrics_dict[name+'auprc_avg'] = average_precision_score(all_targets_2d, probs)\n",
    "            metrics_dict[name+'auc_1'] = roc_auc_score(all_targets, probs[:,1])\n",
    "            metrics_dict[name+'auprc_1'] = average_precision_score(all_targets, probs[:,1])\n",
    "\n",
    "            class_report = classification_report(all_targets,all_preds,output_dict=True) \n",
    "            metrics_dict[name+'precision'] = class_report['1']['precision']\n",
    "            metrics_dict[name+'recall'] = class_report['1']['recall']\n",
    "            metrics_dict[name+'f1-score'] = class_report['1']['f1-score']\n",
    "            metrics_dict[name+'support'] = class_report['1']['support']\n",
    "            metrics_dict[name+'specificity'] = class_report['0']['recall']\n",
    "            metrics_dict['epoch'] = epoch\n",
    "        fn = EXP_NAME+'_metrics.csv'\n",
    "        dir = os.path.join(checkpoints_dir, 'logistic')\n",
    "        df_dict = {k:[v] for k,v in metrics_dict.items()}\n",
    "        if fn not in os.listdir(dir):\n",
    "            pd.DataFrame(df_dict).to_csv(dir+fn, mode='a',header=True)\n",
    "        else:\n",
    "            pd.DataFrame(df_dict).to_csv(dir+fn, mode='a',header=False)\n",
    "        mcc = metrics_dict['val_mcc']\n",
    "\n",
    "        print(\"Iteration: {}. Loss: {}. Accuracy: {}. MCC: {}\".format(epoch, loss.item(), accuracy, mcc))\n",
    "        if mcc>best_mcc:\n",
    "            best_mcc = mcc\n",
    "            print('saving', epoch)\n",
    "            save(bow_nn_model, os.path.join(checkpoints_dir, 'logistic'), 'best', epoch)\n",
    "            \n",
    "print(\"Time taken to train the model: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cbbdc-7dfa-4ad6-8abc-8c147f80faad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
